### Carregando os pacotes necessários
# Aqui, carregamos as bibliotecas usadas para manipulação de dados, visualização e análise textual
library(data.table)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(ggwordcloud)
library(tm)
library(readxl)
library(writexl)

### Carregar a base de dados do OpenAlex
# O caminho do arquivo precisa ser ajustado para o local correto
df_rev_alim <- read_xlsx("caminho_para_arquivo.xlsx")

### Filtragem de artigos relevantes
# Definição de palavras-chave para filtrar artigos de interesse
keywords <- c("direito à alimentação", "direito humano à alimentação", "direito social à alimentação",
              "segurança alimentar", "segurança nutricional", "segurança alimentar e nutricional", 
              "alimentação adequada", "soberania alimentar")

# Criando um padrão regex para filtrar artigos pelo abstract
pattern <- paste(keywords, collapse = "|")

df_rev_alim1 <- df_rev_alim %>%  
  filter(grepl(pattern, abstract, ignore.case = TRUE)) %>% 
  mutate(n_author = str_count(authorships.raw_author_name, "\\|") + 1) # Conta o número de autores

# Criando uma base filtrada para revisão narrativa (artigos com pelo menos 10 citações)
df_rev_alim2 <- df_rev_alim1 %>% 
  filter(!is.na(publication_year)) %>% 
  filter(as.numeric(cited_by_count) >= 10)

# Salvando a base filtrada em um arquivo Excel
write_xlsx(df_rev_alim2, "art_rev_narr_dt_alim.xlsx")

### Geração de gráficos exploratórios
# Quantidade de artigos por ano
ggplot(df_rev_alim1 %>%  
         count(publication_year), 
       aes(x = publication_year, y = n)) + 
  geom_col(fill = "steelblue") + 
  theme_minimal() + 
  labs(x = "Ano", y = "Total de artigos", title = "Produção acadêmica por ano")

# Evolução temporal da produção
ggplot(df_rev_alim1 %>%  
         count(publication_year), 
       aes(x = publication_year, y = n)) + 
  geom_point(color = "darkred") + 
  geom_line(group = 1, color = "darkred") + 
  theme_minimal() + 
  labs(x = "Ano", y = "Total de artigos", title = "Evolução da produção acadêmica")

# Total de citações por ano
ggplot(df_rev_alim1, aes(x = as.factor(publication_year), y = as.numeric(cited_by_count))) + 
  geom_col(fill = "forestgreen") + 
  theme_minimal() + 
  labs(x = "Ano", y = "Total de citações", title = "Citações acumuladas por ano")

### Análise da relação entre citações e impacto
ggplot(df_rev_alim1, aes(x = as.numeric(fwci), y = as.numeric(cited_by_count))) + 
  geom_point(color = "darkblue", alpha = 0.7) + 
  geom_smooth(method = "lm", color = "red") + 
  theme_minimal() + 
  labs(x = "Field Weighted Citation Impact (FWCI)", y = "Citações", title = "Correlação entre impacto e citações")

### Processamento de texto para nuvem de palavras
# Criando um corpus a partir do abstract dos artigos
corpus <- Corpus(VectorSource(df_rev_alim1$abstract))

# Limpeza do texto
corpus <- corpus %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, stopwords("portuguese"))

# Criando a matriz de termos e extraindo frequências
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
df_words <- data.frame(word = names(word_freqs), freq = word_freqs)

# Geração da nuvem de palavras
df_words %>% 
  top_n(100, freq) %>%  
  ggplot(aes(label = word, size = freq)) + 
  geom_text_wordcloud() + 
  theme_minimal() +
  labs(title = "Nuvem de palavras dos abstracts")

### Salvando bases de dados intermediárias
write_csv(df_rev_alim1, "base_filtrada.csv")
write_csv(df_rev_alim2, "base_revisao_narrativa.csv")
